{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtWzro6K6AHm"
      },
      "source": [
        "**SARCASM DETECTION WITH 8 MACHINE LEARNING ALGORITHMS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_m4PC3FT6AHq"
      },
      "source": [
        "**What we have to do?** \\\n",
        "    **We have a dataset of sarcastic headlines from two news websites** \\\n",
        "    **There is each dataset, which consists of three attributes:** \\\n",
        "        - is_sarcastic: 1 if the dataset is sarcastic, 0 otherwise\\\n",
        "        - headline: the headline of the news article\\\n",
        "        - article_link: Link to the original news article\n",
        "        \n",
        "  **We need to find out which news items are sarcastic and which are not.**\\\n",
        "  **For this task we need to work with the following plan:**\n",
        "  \n",
        "    >>> Read and clean the data\n",
        "    >>> Find and visualise most common words\n",
        "    >>> Identify and visualise stop words\n",
        "    >>> Preprocess text\n",
        "    >>> Define classification models\n",
        "    >>> Retrieve and save the best model\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-Z5Gbr76gJX"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9XS3zKc6AHr"
      },
      "source": [
        "**Import library necessary**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3o-jLZaj6AHs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zvJmHlO6AHt",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoLfpc2l6AHt"
      },
      "source": [
        "**Reading and Cleaning the Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9wUbTSJ6AHu"
      },
      "outputs": [],
      "source": [
        "# read the file\n",
        "data = pd.read_json(\"/content/drive/MyDrive/Sarcasm_Headlines_Dataset.json\", lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QeIBfpBS6AHu"
      },
      "outputs": [],
      "source": [
        "# view the data\n",
        "data.head(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zb9tfgrh6AHv"
      },
      "outputs": [],
      "source": [
        "# shape of the data\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M088gD8L6AHv"
      },
      "outputs": [],
      "source": [
        "data_len = data['headline'].apply(lambda x: len(x.split(' '))).sum()\n",
        "print(f'We have {data_len} words in the headline')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNhmJBNs6AHv"
      },
      "outputs": [],
      "source": [
        "# check the columns names\n",
        "data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBjXPgbG6AHw"
      },
      "outputs": [],
      "source": [
        "# check the data types in the columns\n",
        "data.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdgbU5CX6AHx"
      },
      "outputs": [],
      "source": [
        "#checking the unique values in 'is_sarcastic' column\n",
        "data.is_sarcastic.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEcC9WNv6AHx"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2NU981n6AHy"
      },
      "outputs": [],
      "source": [
        "#checking the value counts in 'is_sarcastic' column\n",
        "data.is_sarcastic.value_counts()\n",
        "sns.countplot(data['is_sarcastic'].value_counts())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMmnZopz6AHz"
      },
      "outputs": [],
      "source": [
        "# check the null values in data\n",
        "data.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ub0xlNpG6AHz"
      },
      "outputs": [],
      "source": [
        "#drop 'article_link' column\n",
        "data = data.drop('article_link', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48lSGMT76AH0"
      },
      "outputs": [],
      "source": [
        "#ckeck the data\n",
        "data.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-YYfl5o6AH0",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#import necessary library\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "set_stopwords = set(stopwords.words(\"english\"))\n",
        "\n",
        "\n",
        "def clean_txt(text): # define the fuction with tokenization/string cleaning for all datasets\n",
        "\n",
        "    text = re.sub(r\"[^A-Za-z,!?]\", \" \", text)\n",
        "    text = re.sub(r'\\[[^]]*\\]',\" \", text)\n",
        "    text = re.sub(r\"\\'s\", \"\", text)\n",
        "    text = re.sub(r\"\\'t\", \"\", text )\n",
        "    text = re.sub(r\"\\'re\", \"\",text)\n",
        "    text = re.sub(r\"\\'d\", \"\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" \",text)\n",
        "    text = re.sub(r\",\", \" \", text)\n",
        "    text = re.sub(r\"\\(\", \" \", text)\n",
        "    text = re.sub(r\"\\)\", \" \", text)\n",
        "    text = re.sub(r\"\\'\", \" \", text)\n",
        "    text = re.sub(r\"aa\", \"\", text)\n",
        "    text = re.sub(r\"zz\", \"\", text)\n",
        "    text = re.sub(r\"[0-9]\", ' ', text)\n",
        "    text = text.lower()\n",
        "    text = ' '.join(word for word in text.split() if word not in set_stopwords)\n",
        "    return text\n",
        "\n",
        "data['headline'] = data['headline'].apply(clean_txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBL6fYKK6AH1"
      },
      "outputs": [],
      "source": [
        "data_clean_len = data['headline'].apply(lambda x: len(x.split(' '))).sum()\n",
        "print(f'After text cleaning we have only {data_clean_len} words to work with')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJl4g6eW6AH2"
      },
      "source": [
        "*Most common words*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBoLTs2x6AH2"
      },
      "outputs": [],
      "source": [
        "from collections import Counter #import Counter for finding most common words\n",
        "import seaborn as sns #import searbon for vizualization result\n",
        "\n",
        "text = data['headline']\n",
        "words = text.str.split(expand=True).unstack()\n",
        "result_count = Counter(words).most_common()\n",
        "result_df = pd.DataFrame(result_count).reset_index().drop(0) #converting to Dataframe and drop the Nones values\n",
        "#result_df\n",
        "#vizualize result\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "f, ax = plt.subplots(figsize=(15, 15))\n",
        "sns.barplot(y=result_df[0][0:30], x=result_df[1][0:30], data=result_df, palette=None)\n",
        "plt.ylabel('Words', color=\"blue\")  # Add an x-label to the axes.\n",
        "plt.xlabel('Count', color=\"blue\")  # Add a y-label to the axes.\n",
        "plt.title(\"Frequent Occuring words in Headlines\", color=\"blue\")\n",
        "plt.xticks(rotation=50);\n",
        "ax.tick_params(axis='x', colors='black')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dohEIbQV6AH3"
      },
      "source": [
        "**Finding most common words in 'is_sarcastic' column**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azSgnk2d6AH4"
      },
      "outputs": [],
      "source": [
        "#create DataFrame for sarcastic words\n",
        "sarcastic = pd.DataFrame(data[data['is_sarcastic']==1]['headline'].str.split(expand=True).unstack().value_counts()).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_b-ZzrP36AH5"
      },
      "outputs": [],
      "source": [
        "#create DataFrame for non_sarcastic words\n",
        "non_sarcastic = pd.DataFrame(data[data['is_sarcastic']==0]['headline'].str.split(expand=True).unstack().value_counts()).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYVz0EEU6AH5"
      },
      "outputs": [],
      "source": [
        "#vizualize result\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "f, ax = plt.subplots(figsize=(15, 10))\n",
        "sns.barplot(y=sarcastic['index'][0:30], x=sarcastic[0][0:30], data=result_df, palette=None)\n",
        "plt.ylabel('Words', color=\"blue\")  # Add an x-label to the axes.\n",
        "plt.xlabel('Count', color=\"blue\")  # Add a y-label to the axes.\n",
        "plt.title(\"Frequent Occuring Sarcastic Words in Headlines\", color=\"blue\")\n",
        "plt.xticks(rotation=70);\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXiKWbsc6AH6"
      },
      "outputs": [],
      "source": [
        "#vizualize result\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "f, ax = plt.subplots(figsize=(15, 10))\n",
        "sns.barplot(y=non_sarcastic['index'][0:30], x=non_sarcastic[0][0:30], data=result_df, palette=None)\n",
        "plt.ylabel('Words', color=\"blue\")  # Add an x-label to the axes.\n",
        "plt.xlabel('Count', color=\"blue\")  # Add a y-label to the axes.\n",
        "plt.title(\"Frequent Occuring Non_Sarcastic Words in Headlines\", color=\"blue\")\n",
        "plt.xticks(rotation=70);\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_GNH13c6AH7"
      },
      "source": [
        "**WordCloud Vizualization with StopWords**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2KBiM9n6AH7"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopwords = stopwords.words('english')\n",
        "\n",
        "sarcasctic_2 = [every_word.lower() for every_word in sarcastic['index']]\n",
        "\n",
        "sarc_nonstop = [word for word in sarcasctic_2 if word not in stopwords]\n",
        "\n",
        "non_sarcasctic_2 = [every_word.lower() for every_word in non_sarcastic['index']]\n",
        "\n",
        "non_sarc_nonstop = [word for word in non_sarcasctic_2 if word not in stopwords]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDziJACH6AH7"
      },
      "outputs": [],
      "source": [
        "!pip install wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETt0o3NO6AH8"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "wordcloud = WordCloud(width=1000, height=500,\n",
        "                      max_words=300, min_font_size = 10,\n",
        "                      background_color=\"black\",\n",
        "                      stopwords = stopwords,\n",
        "                      ).generate(' ' .join(word for word in sarc_nonstop))\n",
        "\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.title('Wordcloud of Sarcactic Words', color=\"black\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5Qto4Kp6AH8"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "wordcloud = WordCloud(width=1000, height=500,\n",
        "                      max_words=300, min_font_size = 10,\n",
        "                      background_color=\"black\",\n",
        "                      stopwords = stopwords,\n",
        "                      ).generate(' ' .join(word for word in non_sarc_nonstop))\n",
        "\n",
        "plt.imshow(wordcloud, interpolation='spline36')\n",
        "plt.title('Wordcloud of Non_Sarcactic Words', color=\"black\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBoFHMYi6AH8"
      },
      "source": [
        "***Text pre-processing***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsHyM_lk6AH8"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer()\n",
        "X_cv = cv.fit_transform(data['headline']).toarray()\n",
        "y = data.iloc[:, -1].values\n",
        "cv.get_feature_names_out()\n",
        "df = pd.DataFrame(X_cv, columns=cv.get_feature_names_out())\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7pOV1bG6AH9"
      },
      "source": [
        "**Split text to train and test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_F-TxRI6AH9"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split # import library for train_test_split\n",
        "X = text\n",
        "y = data.is_sarcastic\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state = 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CNuS7Zt6AH9"
      },
      "source": [
        "**Multinomial Naive Bayes Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FELTO16m6AH9"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
        "\n",
        "\n",
        "pipe1 = Pipeline([('vectr', CountVectorizer(analyzer='word', preprocessor=None, min_df=1,)),\n",
        "                   ('MNB', MultinomialNB()),])\n",
        "\n",
        "parameters1 = {'vectr__ngram_range': [(1,1),(1,2),(1,3)],\n",
        "                    'MNB__alpha': [0.05,0.1,0.5,1.0,2.0,4.0],\n",
        "                'MNB__fit_prior': [True]}\n",
        "gridMNB = GridSearchCV(pipe1, parameters1 ,cv=7,n_jobs=-1, verbose=3)\n",
        "gridMNB.fit(X_train, y_train)\n",
        "\n",
        "%time y_pred1 = gridMNB.predict(X_test)\n",
        "\n",
        "#getting the best accuracy and parameters\n",
        "print('MNB_Train Accuracy : %.3f'%gridMNB.best_estimator_.score(X_train, y_train))\n",
        "print('MNB_Test Accuracy : %.3f'%gridMNB.best_estimator_.score(X_test, y_test))\n",
        "print('MNB_Best Accuracy Through Grid Search : %.3f'%gridMNB.best_score_)\n",
        "print('MNB_Best Parameters : ',gridMNB.best_params_)\n",
        "print(15*'--->--->')\n",
        "print('classification_report: \\n', classification_report(y_test, y_pred1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwYS7Z2-6AH9"
      },
      "source": [
        "**Stochastic Gradient Descent Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6jWBxuV6AIP"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "pipe2 = Pipeline([('vectr', CountVectorizer(analyzer='word', preprocessor=None, min_df=1)),\n",
        "                  ('tfidf', TfidfTransformer()),\n",
        "                  ('SGD', SGDClassifier(loss='modified_huber', penalty='l2',alpha=0.0001,shuffle=True,\n",
        "                                      learning_rate='optimal',random_state=None, max_iter=100,)),\n",
        "                   ])\n",
        "parameters2 = {'vectr__ngram_range': [(1,1),(1,2)],\n",
        "                    'SGD__alpha': [0.0001,0.01,1,10],\n",
        "                'SGD__max_iter': [1,10,100,1000],\n",
        "                  'SGD__loss': ['modified_huber'],\n",
        "                  'SGD__penalty': ['l2']}\n",
        "\n",
        "\n",
        "gridSGD = GridSearchCV(pipe2, parameters2 ,cv=8, n_jobs=-1, verbose=3)\n",
        "gridSGD.fit(X_train, y_train)\n",
        "\n",
        "%time y_pred2 = gridSGD.predict(X_test)\n",
        "\n",
        "print('SGD_Train Accuracy : %.3f'%gridSGD.best_estimator_.score(X_train, y_train))\n",
        "print('SGD_Test Accuracy : %.3f'%gridSGD.best_estimator_.score(X_test, y_test))\n",
        "print('SGD_Best Accuracy Through Grid Search : %.3f'%gridSGD.best_score_)\n",
        "print('SGD_Best Parameters : ',gridSGD.best_params_)\n",
        "print(15*'--->--->')\n",
        "print('classification_report: \\n',  classification_report(y_test, y_pred2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LL5pcgvN6AIQ"
      },
      "source": [
        "**KNeighbors Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiubHaIK6AIQ",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "pipe3 = Pipeline([('vectr', CountVectorizer(analyzer='word', preprocessor=None, min_df=1)),\n",
        "                  ('tfidf', TfidfTransformer()),\n",
        "                  ('KN', KNeighborsClassifier(n_neighbors=5, algorithm='brute', weights='uniform', metric='minkowski')),\n",
        "                   ])\n",
        "\n",
        "\n",
        "parameters3 = {'vectr__ngram_range': [(1,1),(1,2)],\n",
        "                    'KN__n_neighbors': [15,30,45,60,75,90],\n",
        "                    'KN__p': [2],\n",
        "                    'KN__leaf_size': [10,20]\n",
        "              }\n",
        "\n",
        "\n",
        "gridKN = GridSearchCV(pipe3, parameters3, scoring='accuracy',cv=6, n_jobs=-1, verbose=3)\n",
        "gridKN.fit(X_train, y_train)\n",
        "\n",
        "%time y_pred3 = gridKN.predict(X_test)\n",
        "print('KN_Train Accuracy : %.3f'%gridKN.best_estimator_.score(X_train, y_train))\n",
        "print('KN_Test Accuracy : %.3f'%gridKN.best_estimator_.score(X_test, y_test))\n",
        "print('KN_Best Accuracy Through Grid Search : %.3f'%gridKN.best_score_)\n",
        "print('KN_Best Parameters : ',gridKN.best_params_)\n",
        "print(15*'--->--->')\n",
        "print('classification_report: \\n',  classification_report(y_test, y_pred3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLDfzcMF6AIR"
      },
      "source": [
        "**Logistic Regression Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJ5A4Foy6AIS"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "pipe4 = Pipeline([('vectr', CountVectorizer(analyzer='word', preprocessor=None, min_df=1)),\n",
        "                ('tfidf', TfidfTransformer()),\n",
        "                ('LR', LogisticRegression(penalty='l2',C=1.0,random_state=None,\n",
        "                                          solver='liblinear',intercept_scaling=1, max_iter=100,)),\n",
        "               ])\n",
        "\n",
        "parameters4 = {'vectr__ngram_range': [(1,1),(1,2)],\n",
        "                    'LR__C': [3,4,5,6,7],\n",
        "                    'LR__tol': [0.0001,0.01,0.1],\n",
        "                  'LR__max_iter': [50,75,100]\n",
        "              }\n",
        "\n",
        "gridLR = GridSearchCV(pipe4, parameters4 ,cv=4, verbose=1, n_jobs=-1)\n",
        "gridLR.fit(X_train, y_train)\n",
        "\n",
        "%time y_pred4 = gridLR.predict(X_test)\n",
        "print('LR_Train Accuracy : %.3f'%gridLR.best_estimator_.score(X_train, y_train))\n",
        "print('LR_Test Accuracy : %.3f'%gridLR.best_estimator_.score(X_test, y_test))\n",
        "print('LR_Best Accuracy Through Grid Search : %.3f'%gridLR.best_score_)\n",
        "print('LR_Best Parameters : ',gridLR.best_params_)\n",
        "print(15*'--->--->')\n",
        "print('classification_report: \\n',  classification_report(y_test, y_pred4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uu3YEG0D6AIS"
      },
      "source": [
        "**Decision Tree Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2kVkDKH6AIT"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "pipe5 = Pipeline([('vectr', CountVectorizer()),\n",
        "                ('tfidf', TfidfTransformer()),\n",
        "                ('DTree', DecisionTreeClassifier(criterion='gini', splitter='best',random_state=0 ,min_samples_leaf=1)),\n",
        "               ])\n",
        "\n",
        "parameters5 = {'vectr__ngram_range': [(1,1),(1,2)],\n",
        "               'DTree__criterion' : ['gini'],\n",
        "               'DTree__max_features': ['auto'],\n",
        "               'DTree__max_depth': [2, 3, 5, 10, 15],\n",
        "               'DTree__min_samples_split': [2, 3, 5, 7, 9],\n",
        "               'DTree__min_samples_leaf': [1,5,8,11],\n",
        "              }\n",
        "\n",
        "gridDtree = GridSearchCV(pipe5, parameters5 ,cv=5)\n",
        "gridDtree.fit(X_train, y_train)\n",
        "\n",
        "%time y_pred5 = gridDtree.predict(X_test)\n",
        "print('Dtree_Train Accuracy : %.3f'%gridDtree.best_estimator_.score(X_train, y_train))\n",
        "print('Dtree_Test Accuracy : %.3f'%gridDtree.best_estimator_.score(X_test, y_test))\n",
        "print('Dtree_Best Accuracy Through Grid Search : %.3f'%gridDtree.best_score_)\n",
        "print('Dtree_Best Parameters : ',gridDtree.best_params_)\n",
        "print(15*'--->--->')\n",
        "print('classification_report: \\n',  classification_report(y_test, y_pred5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSVXSO_k6AIT"
      },
      "source": [
        "**Random Forest Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giqPpTKi6AIU"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "pipe6 = Pipeline([('vectr', CountVectorizer()),\n",
        "                ('tfidf', TfidfTransformer()),\n",
        "                ('RF', RandomForestClassifier(n_estimators=100,criterion='gini', max_features='auto',\n",
        "                                               random_state=None ,min_samples_leaf=1)),\n",
        "               ])\n",
        "\n",
        "parameters6 = {'vectr__ngram_range': [(1,1),(1,2)],\n",
        "               'RF__n_estimators': [100,200],\n",
        "                'RF__criterion': ['gini'],\n",
        "               'RF__min_samples_split': [2],\n",
        "               'RF__min_samples_leaf': [1],\n",
        "              }\n",
        "\n",
        "gridRF = GridSearchCV(pipe6, parameters6 ,cv=8, verbose=1, n_jobs=-1)\n",
        "gridRF.fit(X_train, y_train)\n",
        "\n",
        "%time y_pred6 = gridRF.predict(X_test)\n",
        "print('RF_Train Accuracy : %.3f'%gridRF.best_estimator_.score(X_train, y_train))\n",
        "print('RF_Test Accuracy : %.3f'%gridRF.best_estimator_.score(X_test, y_test))\n",
        "print('RF_Best Accuracy Through Grid Search : %.3f'%gridRF.best_score_)\n",
        "print('RF_Best Parameters : ',gridRF.best_params_)\n",
        "print(15*'--->--->')\n",
        "print('classification_report: \\n',  classification_report(y_test, y_pred6))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fJUyyj-6AIU"
      },
      "source": [
        "**Support Vector Classification**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VX-8-UCf6AIV"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "pipe7 = Pipeline([('vectr', CountVectorizer()),\n",
        "                ('tfidf', TfidfTransformer()),\n",
        "                ('SVC', SVC(C=1.0, kernel='rbf', max_iter=-1,\n",
        "                                random_state=None ,cache_size=200)),\n",
        "               ])\n",
        "\n",
        "parameters7 = {'vectr__ngram_range': [(1,1),(1,2)],\n",
        "               'SVC__C': [0.1,1,10],\n",
        "                'SVC__kernel': ['rbf'],\n",
        "              'SVC__degree': [1,2,3],\n",
        "               'SVC__cache_size': [50,100,200]\n",
        "              }\n",
        "\n",
        "gridSVC = GridSearchCV(pipe7, parameters7 ,cv=3, verbose=3, n_jobs=-1)\n",
        "gridSVC.fit(X_train, y_train)\n",
        "\n",
        "%time y_pred7 = gridSVC.predict(X_test)\n",
        "print('SVC_Train Accuracy : %.3f'%gridSVC.best_estimator_.score(X_train, y_train))\n",
        "print('SVC_Test Accuracy : %.3f'%gridSVC.best_estimator_.score(X_test, y_test))\n",
        "print('SVC_Best Accuracy Through Grid Search : %.3f'%gridSVC.best_score_)\n",
        "print('SVC_Best Parameters : ',gridSVC.best_params_)\n",
        "print(15*'--->--->')\n",
        "print('classification_report: \\n',  classification_report(y_test, y_pred7))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmlDoQ0e6AIW"
      },
      "source": [
        "**Gradient Boosting Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M129tyxW6AIW"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "\n",
        "pipe8 = Pipeline([('vectr', CountVectorizer(analyzer='word', preprocessor=None, min_df=1)),\n",
        "                  ('tfidf', TfidfTransformer()),\n",
        "                  ('BST', GradientBoostingClassifier(n_estimators=50, learning_rate=1.0, random_state=0)),\n",
        "                   ])\n",
        "\n",
        "\n",
        "parameters8 = {'vectr__ngram_range': [(1,1),(1,2)],\n",
        "                    'BST__n_estimators': [50,100,200],\n",
        "                    'BST__max_depth': [3,4,5],\n",
        "                    'BST__learning_rate': [0.05,0.01,0.5,1.0]\n",
        "              }\n",
        "\n",
        "\n",
        "gridBoost = GridSearchCV(pipe8, parameters8, cv=3, verbose=3, n_jobs=-1)\n",
        "gridBoost.fit(X_train, y_train)\n",
        "\n",
        "%time y_pred8 = gridBoost.predict(X_test)\n",
        "print('Boost_Train Accuracy : %.3f'%gridBoost.best_estimator_.score(X_train, y_train))\n",
        "print('Boost_Test Accuracy : %.3f'%gridBoost.best_estimator_.score(X_test, y_test))\n",
        "print('Boost_Best Accuracy Through Grid Search : %.3f'%gridBoost.best_score_)\n",
        "print('Boost_Best Parameters : ',gridBoost.best_params_)\n",
        "print(15*'--->--->')\n",
        "print('classification_report: \\n',  classification_report(y_test, y_pred7))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7qb0MJx6AIZ"
      },
      "outputs": [],
      "source": [
        "from sklearn import  metrics # import library for getting scores\n",
        "\n",
        "models = []\n",
        "models.append(('MNB', gridMNB.best_estimator_))\n",
        "models.append(('SGD', gridSGD.best_estimator_))\n",
        "models.append(('KN', gridKN.best_estimator_))\n",
        "models.append(('LR', gridLR.best_estimator_))\n",
        "models.append(('Dtree', gridDtree.best_estimator_))\n",
        "models.append(('RF', gridRF.best_estimator_))\n",
        "models.append(('SVC', gridSVC.best_estimator_))\n",
        "models.append(('BST', gridBoost.best_estimator_))\n",
        "precision_score = []\n",
        "recall_score = []\n",
        "f1_score = []\n",
        "accuracy_score = []\n",
        "roc_auc_score = []\n",
        "for name, model in models:\n",
        "    print(name)\n",
        "    print(\"precision_score: {}\".format(metrics.precision_score(y_test , model.predict(X_test), average='weighted') ))\n",
        "    print(\"recall_score: {}\".format( metrics.recall_score(y_test , model.predict(X_test), average='weighted') ))\n",
        "    print(\"f1_score: {}\".format( metrics.f1_score(y_test , model.predict(X_test), average='weighted') ))\n",
        "    print(\"accuracy_score: {}\".format( metrics.accuracy_score(y_test , model.predict(X_test)) ))\n",
        "\n",
        "\n",
        "    precision_score.append(metrics.precision_score(y_test , model.predict(X_test), average='weighted') )\n",
        "    recall_score.append(metrics.recall_score(y_test , model.predict(X_test), average='weighted') )\n",
        "    f1_score.append( metrics.f1_score(y_test , model.predict(X_test), average='weighted') )\n",
        "    accuracy_score.append(metrics.accuracy_score(y_test , model.predict(X_test)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqlLMwCA6AIb"
      },
      "outputs": [],
      "source": [
        "#creat the Data Frame for scores in models\n",
        "import pandas as pd\n",
        "scores = {'precision_score': precision_score,\n",
        "     'recall_score': recall_score,\n",
        "     'f1_score': f1_score,\n",
        "     'accuracy_score' : accuracy_score\n",
        "    }\n",
        "df = pd.DataFrame(data=scores)\n",
        "df.insert(loc=0, column='Model', value=['MNB','SGD','KNN','LogReg','RandFor','DTree','SVC', 'BST'])\n",
        "df.sort_values('accuracy_score',ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U32RBTPR6AIc",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#creat the Data Frame for accuracy scores\n",
        "acc = {\n",
        "     'accuracy_score' : accuracy_score\n",
        "    }\n",
        "df = pd.DataFrame(data=acc)\n",
        "df.insert(loc=0, column='Model', value=['MNB','SGD','KNN','LogReg','RandFor','DTree','SVC','BST'])\n",
        "#df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIj5_AEV6AIe",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (16,10)) #plot the accuracy scores\n",
        "sns.barplot(x=df['Model'], y=df['accuracy_score'], data=df)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9n7KBzl6AIe"
      },
      "outputs": [],
      "source": [
        "# Using counfusion matrix for best model(MultinomialNB )\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "conf_mtx=confusion_matrix(y_pred1,y_test)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(conf_mtx,annot=True,fmt='d',cmap='Blues')\n",
        "plt.title(\"0 - Non_sarcastic     1 - Sarcastic\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GtCRBjy6AIg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}